{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mmh3\n",
      "  Downloading mmh3-2.5.1.tar.gz (9.8 kB)\n",
      "Building wheels for collected packages: mmh3\n",
      "  Building wheel for mmh3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mmh3: filename=mmh3-2.5.1-cp36-cp36m-linux_x86_64.whl size=24922 sha256=31499b3421e0d08f65db4b88d5eabc944ce9893c9aebf4f095a0a561fa47018d\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/cc/3a/98/fc5e7f8e1840cf6dcf2435260b29661db90a0b22dbd2739df6\n",
      "Successfully built mmh3\n",
      "Installing collected packages: mmh3\n",
      "Successfully installed mmh3-2.5.1\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mmh3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os, re, time,datetime, copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import mmh3\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,512)\n",
    "        self.fc2 = nn.Linear(512, 64)\n",
    "        self.fc3 = nn.Linear(64,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "FEATURES_SIZE = 1024\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = Net(input_size=FEATURES_SIZE).to(device)\n",
    "# loss\n",
    "criterion = nn.BCELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_to_b_files, path_to_m_files, features_size=1024):\n",
    "        self.features_size = features_size\n",
    "        b_files = [os.path.join(path_to_b_files, f) for f in os.listdir(path_to_b_files)]\n",
    "        m_files = [os.path.join(path_to_m_files, f) for f in os.listdir(path_to_m_files)]\n",
    "        self.list_files = b_files + m_files\n",
    "        self.length = len(self.list_files)\n",
    "        self.labels = torch.cat((torch.zeros(len(b_files)),\n",
    "                                 torch.ones(len(m_files))),0)\n",
    "        \n",
    "    def _extract_features(self, string, hash_dim, split_regex=rb\"\\s+\"):\n",
    "        tokens = re.split(pattern=split_regex, string=string)\n",
    "        hash_buckets = [(mmh3.hash(w) % hash_dim) for w in tokens]\n",
    "        buckets, counts = np.unique(hash_buckets, return_counts=True)\n",
    "        feature_values = np.zeros(hash_dim)\n",
    "        for bucket, count in zip(buckets, counts):\n",
    "            feature_values[bucket] = count\n",
    "        return feature_values\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.list_files[idx], 'rb') as f:\n",
    "            content = f.read()\n",
    "        data = self._extract_features(content, hash_dim=self.features_size, split_regex=rb\"\\s+\")\n",
    "        return torch.FloatTensor(data), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LOG_INTERVAL = 100\n",
    "VAL_INTERVAL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train_b_files = 'data/html/benign_files/training/'\n",
    "path_to_train_m_files = 'data/html/malicious_files/training/'\n",
    "path_to_validation_b_files = 'data/html/benign_files/validation/'\n",
    "path_to_validation_m_files = 'data/html/malicious_files/validation/'\n",
    "\n",
    "train_dataset = CustomDataset(path_to_train_b_files,\n",
    "                                  path_to_train_m_files,\n",
    "                                  FEATURES_SIZE)\n",
    "train_size = len(train_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                                shuffle=True, num_workers=8)\n",
    "\n",
    "val_dataset = CustomDataset(path_to_validation_b_files,\n",
    "                                path_to_validation_m_files,\n",
    "                                FEATURES_SIZE)\n",
    "\n",
    "val_size = len(val_dataset)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                                shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train(net, device, train_dataloader, val_dataloader):\n",
    "#def train_model(model, train_dataloader, val_dataloader, criterion, optimizer,  num_epochs):    \n",
    "    start = time.time() \n",
    "    best_acc  = 0.0\n",
    "    for epoch in range(EPOCHS):\n",
    "        print('Epoch {}/{}'.format(epoch+1, EPOCHS))\n",
    "        print('-' * 10)\n",
    "        tic = time.time()\n",
    "        \n",
    "        net.train() \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for i, (data, label) in enumerate(train_dataloader):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = criterion(output, label.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "            # loss is a tensor, loss.item() the actual value\n",
    "            # data.size(0): BATCH_SIZE\n",
    "            \n",
    "            pred = output.detach()\n",
    "            running_corrects += ((pred > .5) == label.unsqueeze(1)).sum()\n",
    "\n",
    "\n",
    "\n",
    "            if i % LOG_INTERVAL == 0:\n",
    "              logging.info('[Epoch %d Batch %d] Training_Loss: %f' %\n",
    "                            (epoch+1, i, running_loss/BATCH_SIZE))\n",
    "        elapsed = time.time() - tic\n",
    "        speed = i * BATCH_SIZE / elapsed\n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_acc = running_corrects.double() / train_size\n",
    "\n",
    "        logging.info(' Training: \\tSpeed =%.2f samples/sec \\tTime cost =%f secs \\tLoss %f \\tAccuracy %f',\n",
    "                     speed, elapsed, epoch_loss, epoch_acc)      \n",
    "        \n",
    "\n",
    "        \n",
    "        if (epoch +1) % VAL_INTERVAL == 0:\n",
    "          net.eval()\n",
    "          running_corrects = 0\n",
    "          for data, label in val_dataloader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            \n",
    "            output = net(data)\n",
    "            pred = output.detach()\n",
    "            running_corrects += ((pred > .5) == label.unsqueeze(1)).sum()\n",
    "          val_acc = running_corrects.double() / val_size\n",
    "          # deep copy the model\n",
    "          if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            \n",
    "          logging.info(' Validation: \\tAccuracy: %f' % val_acc)\n",
    "\n",
    "\n",
    "\n",
    "    logging.info('Best validation accuracy: {:4f}' % best_acc)        \n",
    "    logging.info('Total:%f' % (time.time()-start))\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    #Load best model weights\n",
    "    model.load_state_dict(best_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 1 Batch 0] Training_Loss: 0.840272\n",
      "INFO:root:[Epoch 1 Batch 100] Training_Loss: 50.539741\n",
      "INFO:root:[Epoch 1 Batch 200] Training_Loss: 76.215772\n",
      "INFO:root:[Epoch 1 Batch 300] Training_Loss: 96.386491\n",
      "INFO:root:[Epoch 1 Batch 400] Training_Loss: 113.793617\n",
      "INFO:root:[Epoch 1 Batch 500] Training_Loss: 132.389906\n",
      "INFO:root:[Epoch 1 Batch 600] Training_Loss: 150.384531\n",
      "INFO:root:[Epoch 1 Batch 700] Training_Loss: 167.307657\n",
      "INFO:root: Training: \tSpeed =956.67 samples/sec \tTime cost =94.059897 secs \tLoss 0.238474 \tAccuracy 0.914233\n",
      "INFO:root: Validation: \tAccuracy: 0.893000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 2 Batch 0] Training_Loss: 0.086947\n",
      "INFO:root:[Epoch 2 Batch 100] Training_Loss: 15.895203\n",
      "INFO:root:[Epoch 2 Batch 200] Training_Loss: 29.035101\n",
      "INFO:root:[Epoch 2 Batch 300] Training_Loss: 43.056632\n",
      "INFO:root:[Epoch 2 Batch 400] Training_Loss: 57.368421\n",
      "INFO:root:[Epoch 2 Batch 500] Training_Loss: 71.373483\n",
      "INFO:root:[Epoch 2 Batch 600] Training_Loss: 82.869181\n",
      "INFO:root:[Epoch 2 Batch 700] Training_Loss: 94.370836\n",
      "INFO:root: Training: \tSpeed =1266.71 samples/sec \tTime cost =71.037685 secs \tLoss 0.134460 \tAccuracy 0.954200\n",
      "INFO:root: Validation: \tAccuracy: 0.906200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 3 Batch 0] Training_Loss: 0.092000\n",
      "INFO:root:[Epoch 3 Batch 100] Training_Loss: 9.917153\n",
      "INFO:root:[Epoch 3 Batch 200] Training_Loss: 19.426199\n",
      "INFO:root:[Epoch 3 Batch 300] Training_Loss: 30.517117\n",
      "INFO:root:[Epoch 3 Batch 400] Training_Loss: 41.522685\n",
      "INFO:root:[Epoch 3 Batch 500] Training_Loss: 51.904216\n",
      "INFO:root:[Epoch 3 Batch 600] Training_Loss: 62.038521\n",
      "INFO:root:[Epoch 3 Batch 700] Training_Loss: 73.886837\n",
      "INFO:root: Training: \tSpeed =1260.40 samples/sec \tTime cost =71.393120 secs \tLoss 0.105373 \tAccuracy 0.963644\n",
      "INFO:root: Validation: \tAccuracy: 0.898700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 4 Batch 0] Training_Loss: 0.081384\n",
      "INFO:root:[Epoch 4 Batch 100] Training_Loss: 10.243231\n",
      "INFO:root:[Epoch 4 Batch 200] Training_Loss: 18.844510\n",
      "INFO:root:[Epoch 4 Batch 300] Training_Loss: 27.364390\n",
      "INFO:root:[Epoch 4 Batch 400] Training_Loss: 36.689920\n",
      "INFO:root:[Epoch 4 Batch 500] Training_Loss: 45.354515\n",
      "INFO:root:[Epoch 4 Batch 600] Training_Loss: 53.339349\n",
      "INFO:root:[Epoch 4 Batch 700] Training_Loss: 64.337882\n",
      "INFO:root: Training: \tSpeed =868.47 samples/sec \tTime cost =103.611714 secs \tLoss 0.091664 \tAccuracy 0.969633\n",
      "INFO:root: Validation: \tAccuracy: 0.912000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 5 Batch 0] Training_Loss: 0.070947\n",
      "INFO:root:[Epoch 5 Batch 100] Training_Loss: 9.385656\n",
      "INFO:root:[Epoch 5 Batch 200] Training_Loss: 21.568529\n",
      "INFO:root:[Epoch 5 Batch 300] Training_Loss: 30.705186\n",
      "INFO:root:[Epoch 5 Batch 400] Training_Loss: 38.708645\n",
      "INFO:root:[Epoch 5 Batch 500] Training_Loss: 49.348228\n",
      "INFO:root:[Epoch 5 Batch 600] Training_Loss: 60.031523\n",
      "INFO:root:[Epoch 5 Batch 700] Training_Loss: 67.417498\n",
      "INFO:root: Training: \tSpeed =446.13 samples/sec \tTime cost =201.697758 secs \tLoss 0.096084 \tAccuracy 0.970911\n",
      "INFO:root: Validation: \tAccuracy: 0.919700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 6 Batch 0] Training_Loss: 0.050926\n",
      "INFO:root:[Epoch 6 Batch 100] Training_Loss: 6.930441\n",
      "INFO:root:[Epoch 6 Batch 200] Training_Loss: 15.691050\n",
      "INFO:root:[Epoch 6 Batch 300] Training_Loss: 22.602173\n",
      "INFO:root:[Epoch 6 Batch 400] Training_Loss: 28.033179\n",
      "INFO:root:[Epoch 6 Batch 500] Training_Loss: 35.890994\n",
      "INFO:root:[Epoch 6 Batch 600] Training_Loss: 44.094500\n",
      "INFO:root:[Epoch 6 Batch 700] Training_Loss: 51.842434\n",
      "INFO:root: Training: \tSpeed =353.36 samples/sec \tTime cost =254.651176 secs \tLoss 0.073945 \tAccuracy 0.977444\n",
      "INFO:root: Validation: \tAccuracy: 0.920900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 7 Batch 0] Training_Loss: 0.070460\n",
      "INFO:root:[Epoch 7 Batch 100] Training_Loss: 7.332041\n",
      "INFO:root:[Epoch 7 Batch 200] Training_Loss: 14.811651\n",
      "INFO:root:[Epoch 7 Batch 300] Training_Loss: 24.672302\n",
      "INFO:root:[Epoch 7 Batch 400] Training_Loss: 32.372216\n",
      "INFO:root:[Epoch 7 Batch 500] Training_Loss: 40.941403\n",
      "INFO:root:[Epoch 7 Batch 600] Training_Loss: 54.541782\n",
      "INFO:root:[Epoch 7 Batch 700] Training_Loss: 67.087204\n",
      "INFO:root: Training: \tSpeed =345.33 samples/sec \tTime cost =260.570775 secs \tLoss 0.095787 \tAccuracy 0.972311\n",
      "INFO:root: Validation: \tAccuracy: 0.920600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 8 Batch 0] Training_Loss: 0.063554\n",
      "INFO:root:[Epoch 8 Batch 100] Training_Loss: 8.894175\n",
      "INFO:root:[Epoch 8 Batch 200] Training_Loss: 17.684133\n",
      "INFO:root:[Epoch 8 Batch 300] Training_Loss: 25.330633\n",
      "INFO:root:[Epoch 8 Batch 400] Training_Loss: 31.806198\n",
      "INFO:root:[Epoch 8 Batch 500] Training_Loss: 40.604515\n",
      "INFO:root:[Epoch 8 Batch 600] Training_Loss: 47.880516\n",
      "INFO:root:[Epoch 8 Batch 700] Training_Loss: 55.614714\n",
      "INFO:root: Training: \tSpeed =349.29 samples/sec \tTime cost =257.622295 secs \tLoss 0.079255 \tAccuracy 0.978333\n",
      "INFO:root: Validation: \tAccuracy: 0.918000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 9 Batch 0] Training_Loss: 0.070623\n",
      "INFO:root:[Epoch 9 Batch 100] Training_Loss: 7.099462\n",
      "INFO:root:[Epoch 9 Batch 200] Training_Loss: 12.491522\n",
      "INFO:root:[Epoch 9 Batch 300] Training_Loss: 19.206853\n",
      "INFO:root:[Epoch 9 Batch 400] Training_Loss: 25.432422\n",
      "INFO:root:[Epoch 9 Batch 500] Training_Loss: 36.366336\n",
      "INFO:root:[Epoch 9 Batch 600] Training_Loss: 45.444347\n",
      "INFO:root:[Epoch 9 Batch 700] Training_Loss: 52.075326\n",
      "INFO:root: Training: \tSpeed =322.61 samples/sec \tTime cost =278.922453 secs \tLoss 0.074329 \tAccuracy 0.980267\n",
      "INFO:root: Validation: \tAccuracy: 0.922100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 10 Batch 0] Training_Loss: 0.030613\n",
      "INFO:root:[Epoch 10 Batch 100] Training_Loss: 6.253498\n",
      "INFO:root:[Epoch 10 Batch 200] Training_Loss: 13.421359\n",
      "INFO:root:[Epoch 10 Batch 300] Training_Loss: 18.781872\n",
      "INFO:root:[Epoch 10 Batch 400] Training_Loss: 26.132133\n",
      "INFO:root:[Epoch 10 Batch 500] Training_Loss: 34.999702\n",
      "INFO:root:[Epoch 10 Batch 600] Training_Loss: 40.946223\n",
      "INFO:root:[Epoch 10 Batch 700] Training_Loss: 49.466839\n",
      "INFO:root: Training: \tSpeed =360.96 samples/sec \tTime cost =249.289503 secs \tLoss 0.070597 \tAccuracy 0.981189\n",
      "INFO:root: Validation: \tAccuracy: 0.928500\n",
      "INFO:root:Best validation accuracy: {:4f}\n",
      "INFO:root:Total:2151.782611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete in 35m 52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, device, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
