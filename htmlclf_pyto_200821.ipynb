{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"htmlclf_pyto_200821.ipynb","provenance":[{"file_id":"1ZUsQgInmwinizAql9dj0w9_s_90KrDHz","timestamp":1571130534556},{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/quickstart/beginner.ipynb","timestamp":1555437903831}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"hH1UdadsrlGM","colab_type":"text"},"source":["# Neural network HTML classifier with PyTorch, 21.08.2020\n"]},{"cell_type":"code","metadata":{"id":"QTpmZQtVLIo_","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)\n","base_dir = \"gdrive/My Drive/Colab Notebooks/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"40fg_XzLLcaQ","colab_type":"code","colab":{}},"source":["!mkdir -p data\n","!tar -xzf gdrive/My\\ Drive/Colab\\ Notebooks/Data/htmldata.tar.gz -C data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NAj1dMac7C08","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"XM4_Uk5QCIt8","colab_type":"code","colab":{}},"source":["!pip install mmh3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kjh_RouvVZR8","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import os, re, time,datetime, copy\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.utils.data import Dataset, DataLoader\n","\n","import mmh3\n","import logging\n","\n","logging.basicConfig(level=logging.INFO)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5rImQAoJliC","colab_type":"code","colab":{}},"source":["class Net(nn.Module):\n","    def __init__(self, input_size):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(input_size,512)\n","        self.fc2 = nn.Linear(512, 64)\n","        self.fc3 = nn.Linear(64,1)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        x = F.relu(x)\n","        x = self.fc3(x)\n","        x = torch.sigmoid(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nihJmYtkRf8J","colab_type":"code","colab":{}},"source":["FEATURES_SIZE = 1024\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","model = Net(input_size=FEATURES_SIZE).to(device)\n","# loss\n","criterion = nn.BCELoss()\n","# optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qc6TsrBRXf2a","colab_type":"text"},"source":["#Custom Dataset"]},{"cell_type":"code","metadata":{"id":"gqPanpaT0pgR","colab_type":"code","colab":{}},"source":["class CustomDataset(Dataset):\n","    def __init__(self, path_to_b_files, path_to_m_files, features_size=1024):\n","        self.features_size = features_size\n","        b_files = [os.path.join(path_to_b_files, f) for f in os.listdir(path_to_b_files)]\n","        m_files = [os.path.join(path_to_m_files, f) for f in os.listdir(path_to_m_files)]\n","        self.list_files = b_files + m_files\n","        self.length = len(self.list_files)\n","        self.labels = torch.cat((torch.zeros(len(b_files)),\n","                                 torch.ones(len(m_files))),0)\n","        \n","    def _extract_features(self, string, hash_dim, split_regex=rb\"\\s+\"):\n","        tokens = re.split(pattern=split_regex, string=string)\n","        hash_buckets = [(mmh3.hash(w) % hash_dim) for w in tokens]\n","        buckets, counts = np.unique(hash_buckets, return_counts=True)\n","        feature_values = np.zeros(hash_dim)\n","        for bucket, count in zip(buckets, counts):\n","            feature_values[bucket] = count\n","        return feature_values\n","\n","    def __getitem__(self, idx):\n","        with open(self.list_files[idx], 'rb') as f:\n","            content = f.read()\n","        data = self._extract_features(content, hash_dim=self.features_size, split_regex=rb\"\\s+\")\n","        return torch.FloatTensor(data), self.labels[idx]\n","\n","    def __len__(self):\n","        return self.length"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N8HCAR_733CO","colab_type":"text"},"source":["# Parameters"]},{"cell_type":"code","metadata":{"id":"8YbZ5mIVBF2F","colab_type":"code","colab":{}},"source":["BATCH_SIZE = 128\n","EPOCHS = 10\n","LOG_INTERVAL = 100\n","VAL_INTERVAL = 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mKVsyZSt0JYv","colab_type":"text"},"source":["#Dataloaders"]},{"cell_type":"code","metadata":{"id":"0IxKFcV41LPE","colab_type":"code","colab":{}},"source":["path_to_train_b_files = 'data/html/benign_files/training/'\n","path_to_train_m_files = 'data/html/malicious_files/training/'\n","path_to_validation_b_files = 'data/html/benign_files/validation/'\n","path_to_validation_m_files = 'data/html/malicious_files/validation/'\n","\n","train_dataset = CustomDataset(path_to_train_b_files,\n","                                  path_to_train_m_files,\n","                                  FEATURES_SIZE)\n","train_size = len(train_dataset)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                                shuffle=True, num_workers=8)\n","\n","val_dataset = CustomDataset(path_to_validation_b_files,\n","                                path_to_validation_m_files,\n","                                FEATURES_SIZE)\n","\n","val_size = len(val_dataset)\n","\n","val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n","                                shuffle=False, num_workers=8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PgmA3ngpB6sQ","colab_type":"code","colab":{}},"source":["# Function to train the model\n","def train(net, device, train_dataloader, val_dataloader, num_epochs):\n","    start = time.time() \n","    best_acc  = 0.0\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n","        print('-' * 10)\n","        tic = time.time()\n","        \n","        net.train() \n","        running_loss = 0.0\n","        running_corrects = 0\n","        for i, (data, label) in enumerate(train_dataloader):\n","            data, label = data.to(device), label.to(device)\n","            optimizer.zero_grad()\n","            output = net(data)\n","            loss = criterion(output, label.unsqueeze(1))\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * data.size(0)\n","            # loss is a tensor, loss.item() the actual value\n","            # data.size(0): BATCH_SIZE\n","            \n","            pred = output.detach()\n","            running_corrects += ((pred > .5) == label.unsqueeze(1)).sum()\n","\n","            if i >0 and i % LOG_INTERVAL == 0:\n","              logging.info('[Epoch %d Batch %d] Training_Loss: %f' %\n","                            (epoch+1, i, running_loss/(i*BATCH_SIZE)))\n","        elapsed = time.time() - tic\n","        speed = i * BATCH_SIZE / elapsed\n","        epoch_loss = running_loss / len(train_dataset)\n","        epoch_acc = running_corrects.double() / train_size\n","\n","        logging.info(' Training: \\tSpeed =%.2f samples/sec \\tTime cost =%f secs \\tLoss %f \\tAccuracy %f',\n","                     speed, elapsed, epoch_loss, epoch_acc)      \n","                \n","        if (epoch +1) % VAL_INTERVAL == 0:\n","          net.eval()\n","          running_corrects = 0\n","          for data, label in val_dataloader:\n","            data, label = data.to(device), label.to(device)\n","            \n","            output = net(data)\n","            pred = output.detach()\n","            running_corrects += ((pred > .5) == label.unsqueeze(1)).sum()\n","          val_acc = running_corrects.double() / val_size\n","          # deep copy the model\n","          if val_acc > best_acc:\n","            best_acc = val_acc\n","            best_model = copy.deepcopy(model.state_dict())\n","                       \n","          logging.info(' Validation: \\tAccuracy: %f' % val_acc)\n","\n","    logging.info('Best validation accuracy: %4f' % best_acc)        \n","    logging.info('Total:%f' % (time.time()-start))\n","    time_elapsed = time.time() - start\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    #Load best model weights\n","    model.load_state_dict(best_model)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z28yVDriN4-O","colab_type":"code","colab":{}},"source":["model = train(model, device, train_dataloader, val_dataloader, EPOCHS)"],"execution_count":null,"outputs":[]}]}